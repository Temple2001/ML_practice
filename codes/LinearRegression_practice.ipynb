{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression_practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyG2vdNCjzrIZZj303o67z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Temple2001/ML_practice/blob/main/codes/LinearRegression_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wSE-RAxOhoy"
      },
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0J6IaYAHWk"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def numerical_derivative(f, x):\r\n",
        "  delta_x = 1e-4\r\n",
        "  grad = np.zeros_like(x)\r\n",
        "\r\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\r\n",
        "\r\n",
        "  while not it.finished:\r\n",
        "    idx = it.multi_index\r\n",
        "    tmp_val = x[idx]\r\n",
        "    x[idx] = float(tmp_val) + delta_x\r\n",
        "    fx1 = f(x)\r\n",
        "\r\n",
        "    x[idx] = float(tmp_val) - delta_x\r\n",
        "    fx2 = f(x)\r\n",
        "    grad[idx] = (fx1 - fx2) / (2*delta_x)\r\n",
        "\r\n",
        "    x[idx] = tmp_val\r\n",
        "    it.iternext()\r\n",
        "\r\n",
        "  return grad"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDrkXTTJEMXu",
        "outputId": "f0f8f8b4-c984-4f5a-a519-ef41acd3ba7f"
      },
      "source": [
        "x_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0]).reshape(5,1)\r\n",
        "t_data = np.array([2.0, 3.0, 4.0, 5.0, 6.0]).reshape(5,1)\r\n",
        "\r\n",
        "print('x_data.shape = ', x_data.shape, ', t_data.shape = ', t_data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data.shape =  (5, 1) , t_data.shape =  (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8InSoFuE4xm",
        "outputId": "7d6e2e4e-1859-42ea-9c1a-1ed3d597910c"
      },
      "source": [
        "W = np.random.rand(1,1)\r\n",
        "b = np.random.rand(1)\r\n",
        "\r\n",
        "print('W = ', W, ', W.shape = ', W.shape, ', b = ', b, ', b.shape = ', b.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W =  [[0.55508332]] , W.shape =  (1, 1) , b =  [0.54836789] , b.shape =  (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mXAyRG-Fc7H"
      },
      "source": [
        "def loss_func(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "\r\n",
        "  return ( np.sum((t-y)**2) / (len(x)))\r\n",
        "\r\n",
        "def loss_val(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "\r\n",
        "  return ( np.sum((t-y)**2) / (len(x)))\r\n",
        "\r\n",
        "def predict(x):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXN-rJztF72W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac71ae67-d764-4932-c446-2e1dfb21ee10"
      },
      "source": [
        "learning_rate = 1e-2\r\n",
        "\r\n",
        "f = lambda x : loss_func(x_data, t_data)\r\n",
        "\r\n",
        "print('initial loss value = ', loss_val(x_data, t_data), 'initial W = ', W, '\\n', ', b = ', b)\r\n",
        "\r\n",
        "for step in range(8001):\r\n",
        "  W -= learning_rate * numerical_derivative(f, W)\r\n",
        "  b -= learning_rate * numerical_derivative(f, b)\r\n",
        "\r\n",
        "  if (step % 400 == 0):\r\n",
        "    print('step = ', step, ', loss value = ', loss_val(x_data, t_data), ', W = ', W, ', b = ', b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial loss value =  3.5870628239170346 initial W =  [[0.55508332]] \n",
            " , b =  [0.54836789]\n",
            "step =  0 , loss value =  2.1180017730356226 , W =  [[0.68006292]] , b =  [0.57659676]\n",
            "step =  400 , loss value =  0.001297594841720573 , W =  [[1.02339194]] , b =  [0.91556834]\n",
            "step =  800 , loss value =  8.279422224720827e-05 , W =  [[1.00590877]] , b =  [0.9786727]\n",
            "step =  1200 , loss value =  5.282760856564825e-06 , W =  [[1.00149254]] , b =  [0.99461276]\n",
            "step =  1600 , loss value =  3.370713741871358e-07 , W =  [[1.00037701]] , b =  [0.99863919]\n",
            "step =  2000 , loss value =  2.1507146429859164e-08 , W =  [[1.00009523]] , b =  [0.99965626]\n",
            "step =  2400 , loss value =  1.3722830918986414e-09 , W =  [[1.00002406]] , b =  [0.99991317]\n",
            "step =  2800 , loss value =  8.75597741672192e-11 , W =  [[1.00000608]] , b =  [0.99997807]\n",
            "step =  3200 , loss value =  5.586831243095368e-12 , W =  [[1.00000153]] , b =  [0.99999446]\n",
            "step =  3600 , loss value =  3.564728628107527e-13 , W =  [[1.00000039]] , b =  [0.9999986]\n",
            "step =  4000 , loss value =  2.274507604758876e-14 , W =  [[1.0000001]] , b =  [0.99999965]\n",
            "step =  4400 , loss value =  1.4512703154342941e-15 , W =  [[1.00000002]] , b =  [0.99999991]\n",
            "step =  4800 , loss value =  9.259962773900011e-17 , W =  [[1.00000001]] , b =  [0.99999998]\n",
            "step =  5200 , loss value =  5.90840368365256e-18 , W =  [[1.]] , b =  [0.99999999]\n",
            "step =  5600 , loss value =  3.769913443026588e-19 , W =  [[1.]] , b =  [1.]\n",
            "step =  6000 , loss value =  2.40543078643063e-20 , W =  [[1.]] , b =  [1.]\n",
            "step =  6400 , loss value =  1.5348185029551592e-21 , W =  [[1.]] , b =  [1.]\n",
            "step =  6800 , loss value =  9.793072363760907e-23 , W =  [[1.]] , b =  [1.]\n",
            "step =  7200 , loss value =  6.250686909368725e-24 , W =  [[1.]] , b =  [1.]\n",
            "step =  7600 , loss value =  3.9970386943277257e-25 , W =  [[1.]] , b =  [1.]\n",
            "step =  8000 , loss value =  2.5663183525604696e-26 , W =  [[1.]] , b =  [1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoMpTPeRM-nH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecde5af-5a4e-42f3-9059-42bc530dc6c7"
      },
      "source": [
        "test_data = np.array([43])\r\n",
        "predict(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsit8-DNOdK-"
      },
      "source": [
        "# Example 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih4n-VafPlxt"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctSKOHqXP9rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35cd50b-87f4-46de-ed69-aab9518299e2"
      },
      "source": [
        "loaded_data = np.loadtxt('data-01.csv', delimiter=',', dtype=np.float32)\r\n",
        "\r\n",
        "x_data = loaded_data[:, 0:-1]\r\n",
        "t_data = loaded_data[:,[-1]]\r\n",
        "\r\n",
        "print('x_data.ndim = ', x_data.ndim, ', x_data.shape = ', x_data.shape)\r\n",
        "print('t_data.ndim = ', t_data.ndim, ', t_data.shape = ', t_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
            "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOdWyZ7XSMpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d7415b-cdb1-4e4d-debf-d71c31f751bf"
      },
      "source": [
        "W = np.random.rand(3,1)\r\n",
        "b = np.random.rand(1)\r\n",
        "print('W = ', W, ', W.shape = ', W.shape, ',b = ', b, ', b.shape = ', b.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W =  [[0.50765183]\n",
            " [0.09642743]\n",
            " [0.08342387]] , W.shape =  (3, 1) ,b =  [0.79164721] , b.shape =  (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uMgr_wTD08"
      },
      "source": [
        "def loss_func(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def loss_val(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def predict(x):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An-GVeMsUubP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e672561e-5cff-4355-c68d-cdef023e0fcb"
      },
      "source": [
        "learning_rate = 1e-5\r\n",
        "\r\n",
        "f = lambda x : loss_func(x_data, t_data)\r\n",
        "\r\n",
        "print('initial loss value = ', loss_val(x_data, t_data), ', initial W = ', W, '\\n', ', b = ', b)\r\n",
        "\r\n",
        "for step in range(40001):\r\n",
        "  W -= learning_rate * numerical_derivative(f, W)\r\n",
        "  b -= learning_rate * numerical_derivative(f, b)\r\n",
        "\r\n",
        "  if (step % 400 == 0):\r\n",
        "    print('step = ', step, ', loss value = ', loss_val(x_data, t_data), ', W = ', W, ', b = ', b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial loss value =  11628.795691169276 , initial W =  [[0.50765183]\n",
            " [0.09642743]\n",
            " [0.08342387]] \n",
            " , b =  [0.79164721]\n",
            "step =  0 , loss value =  4310.214154825574 , W =  [[0.67969393]\n",
            " [0.26950493]\n",
            " [0.26081229]] , b =  [0.79294429]\n",
            "step =  400 , loss value =  14.394925043016055 , W =  [[0.88936112]\n",
            " [0.51592728]\n",
            " [0.61323477]] , b =  [0.79482456]\n",
            "step =  800 , loss value =  12.543694245744419 , W =  [[0.83756037]\n",
            " [0.49930831]\n",
            " [0.67982066]] , b =  [0.79460591]\n",
            "step =  1200 , loss value =  11.139996584462319 , W =  [[0.79086079]\n",
            " [0.4871671 ]\n",
            " [0.73707411]] , b =  [0.79431304]\n",
            "step =  1600 , loss value =  10.069179166940126 , W =  [[0.74874883]\n",
            " [0.47860578]\n",
            " [0.78637152]] , b =  [0.79395626]\n",
            "step =  2000 , loss value =  9.24740937676927 , W =  [[0.71076439]\n",
            " [0.47288875]\n",
            " [0.82887875]] , b =  [0.79354435]\n",
            "step =  2400 , loss value =  8.613083539253264 , W =  [[0.67649501]\n",
            " [0.46941476]\n",
            " [0.86558413]] , b =  [0.79308483]\n",
            "step =  2800 , loss value =  8.12069229131067 , W =  [[0.64557068]\n",
            " [0.46769364]\n",
            " [0.89732617]] , b =  [0.79258412]\n",
            "step =  3200 , loss value =  7.736428130582952 , W =  [[0.61765931]\n",
            " [0.46732694]\n",
            " [0.9248169 ]] , b =  [0.79204775]\n",
            "step =  3600 , loss value =  7.435030717219558 , W =  [[0.59246267]\n",
            " [0.46799185]\n",
            " [0.94866158]] , b =  [0.79148043]\n",
            "step =  4000 , loss value =  7.197513397679992 , W =  [[0.56971278]\n",
            " [0.46942789]\n",
            " [0.96937512]] , b =  [0.79088623]\n",
            "step =  4400 , loss value =  7.009518607613969 , W =  [[0.54916876]\n",
            " [0.47142581]\n",
            " [0.98739609]] , b =  [0.79026865]\n",
            "step =  4800 , loss value =  6.860123271621363 , W =  [[0.53061394]\n",
            " [0.47381839]\n",
            " [1.0030984 ]] , b =  [0.78963071]\n",
            "step =  5200 , loss value =  6.740967165491427 , W =  [[0.5138534 ]\n",
            " [0.47647285]\n",
            " [1.01680122]] , b =  [0.78897499]\n",
            "step =  5600 , loss value =  6.64561384251482 , W =  [[0.49871169]\n",
            " [0.47928459]\n",
            " [1.02877728]] , b =  [0.78830375]\n",
            "step =  6000 , loss value =  6.569079647134823 , W =  [[0.48503081]\n",
            " [0.4821719 ]\n",
            " [1.03925991]] , b =  [0.78761893]\n",
            "step =  6400 , loss value =  6.507484707958813 , W =  [[0.47266846]\n",
            " [0.48507176]\n",
            " [1.04844901]] , b =  [0.78692222]\n",
            "step =  6800 , loss value =  6.457792841389785 , W =  [[0.46149642]\n",
            " [0.4879362 ]\n",
            " [1.05651603]] , b =  [0.78621507]\n",
            "step =  7200 , loss value =  6.417616571539145 , W =  [[0.45139912]\n",
            " [0.49072946]\n",
            " [1.06360822]] , b =  [0.78549877]\n",
            "step =  7600 , loss value =  6.385070083491279 , W =  [[0.44227235]\n",
            " [0.49342556]\n",
            " [1.06985223]] , b =  [0.78477441]\n",
            "step =  8000 , loss value =  6.358657652033667 , W =  [[0.43402217]\n",
            " [0.4960063 ]\n",
            " [1.07535714]] , b =  [0.78404296]\n",
            "step =  8400 , loss value =  6.337188474473746 , W =  [[0.4265638 ]\n",
            " [0.49845971]\n",
            " [1.08021701]] , b =  [0.78330526]\n",
            "step =  8800 , loss value =  6.31971127107256 , W =  [[0.41982079]\n",
            " [0.50077869]\n",
            " [1.08451309]] , b =  [0.78256206]\n",
            "step =  9200 , loss value =  6.305463773511589 , W =  [[0.41372412]\n",
            " [0.50295994]\n",
            " [1.08831568]] , b =  [0.78181399]\n",
            "step =  9600 , loss value =  6.293833494419787 , W =  [[0.4082115 ]\n",
            " [0.50500309]\n",
            " [1.09168564]] , b =  [0.78106162]\n",
            "step =  10000 , loss value =  6.284327096734846 , W =  [[0.40322671]\n",
            " [0.50691001]\n",
            " [1.09467581]] , b =  [0.78030545]\n",
            "step =  10400 , loss value =  6.276546358212729 , W =  [[0.39871897]\n",
            " [0.50868423]\n",
            " [1.09733206]] , b =  [0.77954592]\n",
            "step =  10800 , loss value =  6.2701692232676 , W =  [[0.39464243]\n",
            " [0.51033047]\n",
            " [1.09969435]] , b =  [0.77878341]\n",
            "step =  11200 , loss value =  6.264934801164697 , W =  [[0.39095569]\n",
            " [0.51185429]\n",
            " [1.10179747]] , b =  [0.77801826]\n",
            "step =  11600 , loss value =  6.260631441933821 , W =  [[0.38762132]\n",
            " [0.51326181]\n",
            " [1.10367182]] , b =  [0.77725077]\n",
            "step =  12000 , loss value =  6.257087224732644 , W =  [[0.38460554]\n",
            " [0.51455945]\n",
            " [1.10534393]] , b =  [0.77648122]\n",
            "step =  12400 , loss value =  6.25416234614067 , W =  [[0.38187781]\n",
            " [0.51575379]\n",
            " [1.10683707]] , b =  [0.77570982]\n",
            "step =  12800 , loss value =  6.251743011299514 , W =  [[0.37941053]\n",
            " [0.51685142]\n",
            " [1.10817162]] , b =  [0.7749368]\n",
            "step =  13200 , loss value =  6.249736518577435 , W =  [[0.37717878]\n",
            " [0.5178588 ]\n",
            " [1.10936547]] , b =  [0.77416233]\n",
            "step =  13600 , loss value =  6.248067295563693 , W =  [[0.37516   ]\n",
            " [0.51878226]\n",
            " [1.11043437]] , b =  [0.77338659]\n",
            "step =  14000 , loss value =  6.246673695846011 , W =  [[0.37333383]\n",
            " [0.51962787]\n",
            " [1.11139217]] , b =  [0.77260971]\n",
            "step =  14400 , loss value =  6.245505405989165 , W =  [[0.37168186]\n",
            " [0.52040144]\n",
            " [1.11225108]] , b =  [0.77183183]\n",
            "step =  14800 , loss value =  6.244521343229632 , W =  [[0.37018742]\n",
            " [0.5211085 ]\n",
            " [1.1130219 ]] , b =  [0.77105306]\n",
            "step =  15200 , loss value =  6.243687948721213 , W =  [[0.36883548]\n",
            " [0.52175425]\n",
            " [1.11371415]] , b =  [0.77027351]\n",
            "step =  15600 , loss value =  6.242977800280838 , W =  [[0.36761242]\n",
            " [0.52234358]\n",
            " [1.11433629]] , b =  [0.76949327]\n",
            "step =  16000 , loss value =  6.242368483675233 , W =  [[0.36650593]\n",
            " [0.52288109]\n",
            " [1.11489578]] , b =  [0.76871242]\n",
            "step =  16400 , loss value =  6.241841673452235 , W =  [[0.36550489]\n",
            " [0.52337105]\n",
            " [1.11539927]] , b =  [0.76793104]\n",
            "step =  16800 , loss value =  6.241382383843271 , W =  [[0.36459924]\n",
            " [0.52381743]\n",
            " [1.11585265]] , b =  [0.76714919]\n",
            "step =  17200 , loss value =  6.240978357865274 , W =  [[0.36377987]\n",
            " [0.52422392]\n",
            " [1.11626116]] , b =  [0.76636693]\n",
            "step =  17600 , loss value =  6.240619568842545 , W =  [[0.36303855]\n",
            " [0.52459392]\n",
            " [1.11662946]] , b =  [0.76558431]\n",
            "step =  18000 , loss value =  6.240297813460522 , W =  [[0.36236785]\n",
            " [0.5249306 ]\n",
            " [1.11696172]] , b =  [0.76480139]\n",
            "step =  18400 , loss value =  6.240006379403776 , W =  [[0.36176102]\n",
            " [0.52523684]\n",
            " [1.11726164]] , b =  [0.7640182]\n",
            "step =  18800 , loss value =  6.2397397738101485 , W =  [[0.36121199]\n",
            " [0.52551532]\n",
            " [1.11753253]] , b =  [0.76323478]\n",
            "step =  19200 , loss value =  6.239493501342812 , W =  [[0.36071523]\n",
            " [0.52576849]\n",
            " [1.11777734]] , b =  [0.76245117]\n",
            "step =  19600 , loss value =  6.239263882766103 , W =  [[0.36026577]\n",
            " [0.5259986 ]\n",
            " [1.11799873]] , b =  [0.76166739]\n",
            "step =  20000 , loss value =  6.239047906597956 , W =  [[0.3598591 ]\n",
            " [0.52620771]\n",
            " [1.11819905]] , b =  [0.76088349]\n",
            "step =  20400 , loss value =  6.238843107784652 , W =  [[0.35949115]\n",
            " [0.52639771]\n",
            " [1.11838042]] , b =  [0.76009947]\n",
            "step =  20800 , loss value =  6.238647468459166 , W =  [[0.35915822]\n",
            " [0.52657032]\n",
            " [1.11854474]] , b =  [0.75931537]\n",
            "step =  21200 , loss value =  6.238459336751192 , W =  [[0.35885698]\n",
            " [0.52672712]\n",
            " [1.11869371]] , b =  [0.7585312]\n",
            "step =  21600 , loss value =  6.238277360357335 , W =  [[0.35858442]\n",
            " [0.52686955]\n",
            " [1.11882887]] , b =  [0.75774698]\n",
            "step =  22000 , loss value =  6.238100432182189 , W =  [[0.3583378 ]\n",
            " [0.52699892]\n",
            " [1.11895157]] , b =  [0.75696273]\n",
            "step =  22400 , loss value =  6.237927645852796 , W =  [[0.35811465]\n",
            " [0.52711642]\n",
            " [1.11906306]] , b =  [0.75617847]\n",
            "step =  22800 , loss value =  6.237758259310487 , W =  [[0.35791273]\n",
            " [0.52722316]\n",
            " [1.11916443]] , b =  [0.7553942]\n",
            "step =  23200 , loss value =  6.2375916650110685 , W =  [[0.35773003]\n",
            " [0.52732011]\n",
            " [1.11925669]] , b =  [0.75460994]\n",
            "step =  23600 , loss value =  6.237427365533095 , W =  [[0.35756472]\n",
            " [0.52740819]\n",
            " [1.11934072]] , b =  [0.7538257]\n",
            "step =  24000 , loss value =  6.237264953611173 , W =  [[0.35741513]\n",
            " [0.52748822]\n",
            " [1.11941734]] , b =  [0.75304149]\n",
            "step =  24400 , loss value =  6.2371040957912065 , W =  [[0.35727978]\n",
            " [0.52756094]\n",
            " [1.11948726]] , b =  [0.75225732]\n",
            "step =  24800 , loss value =  6.236944519049984 , W =  [[0.35715731]\n",
            " [0.52762703]\n",
            " [1.11955114]] , b =  [0.75147319]\n",
            "step =  25200 , loss value =  6.23678599984109 , W =  [[0.35704649]\n",
            " [0.52768711]\n",
            " [1.11960958]] , b =  [0.75068912]\n",
            "step =  25600 , loss value =  6.2366283551270945 , W =  [[0.35694621]\n",
            " [0.52774175]\n",
            " [1.11966308]] , b =  [0.7499051]\n",
            "step =  26000 , loss value =  6.23647143503748 , W =  [[0.35685548]\n",
            " [0.52779144]\n",
            " [1.11971215]] , b =  [0.74912115]\n",
            "step =  26400 , loss value =  6.236315116857497 , W =  [[0.35677337]\n",
            " [0.52783665]\n",
            " [1.1197572 ]] , b =  [0.74833728]\n",
            "step =  26800 , loss value =  6.236159300106826 , W =  [[0.35669908]\n",
            " [0.52787781]\n",
            " [1.11979862]] , b =  [0.74755348]\n",
            "step =  27200 , loss value =  6.236003902510211 , W =  [[0.35663185]\n",
            " [0.52791528]\n",
            " [1.11983677]] , b =  [0.74676976]\n",
            "step =  27600 , loss value =  6.235848856698249 , W =  [[0.35657102]\n",
            " [0.52794943]\n",
            " [1.11987196]] , b =  [0.74598612]\n",
            "step =  28000 , loss value =  6.235694107506302 , W =  [[0.35651598]\n",
            " [0.52798055]\n",
            " [1.11990447]] , b =  [0.74520257]\n",
            "step =  28400 , loss value =  6.23553960976295 , W =  [[0.35646617]\n",
            " [0.52800894]\n",
            " [1.11993457]] , b =  [0.74441911]\n",
            "step =  28800 , loss value =  6.235385326478973 , W =  [[0.3564211 ]\n",
            " [0.52803486]\n",
            " [1.11996248]] , b =  [0.74363574]\n",
            "step =  29200 , loss value =  6.2352312273649 , W =  [[0.35638031]\n",
            " [0.52805852]\n",
            " [1.11998842]] , b =  [0.74285248]\n",
            "step =  29600 , loss value =  6.2350772876168685 , W =  [[0.35634341]\n",
            " [0.52808016]\n",
            " [1.12001258]] , b =  [0.74206931]\n",
            "step =  30000 , loss value =  6.234923486922607 , W =  [[0.35631001]\n",
            " [0.52809995]\n",
            " [1.12003512]] , b =  [0.74128624]\n",
            "step =  30400 , loss value =  6.234769808647813 , W =  [[0.35627979]\n",
            " [0.52811808]\n",
            " [1.1200562 ]] , b =  [0.74050327]\n",
            "step =  30800 , loss value =  6.234616239169396 , W =  [[0.35625245]\n",
            " [0.5281347 ]\n",
            " [1.12007597]] , b =  [0.7397204]\n",
            "step =  31200 , loss value =  6.234462767330079 , W =  [[0.3562277 ]\n",
            " [0.52814994]\n",
            " [1.12009454]] , b =  [0.73893765]\n",
            "step =  31600 , loss value =  6.234309383991473 , W =  [[0.35620531]\n",
            " [0.52816396]\n",
            " [1.12011203]] , b =  [0.738155]\n",
            "step =  32000 , loss value =  6.234156081668427 , W =  [[0.35618505]\n",
            " [0.52817685]\n",
            " [1.12012855]] , b =  [0.73737246]\n",
            "step =  32400 , loss value =  6.234002854229269 , W =  [[0.35616671]\n",
            " [0.52818872]\n",
            " [1.12014419]] , b =  [0.73659002]\n",
            "step =  32800 , loss value =  6.2338496966508385 , W =  [[0.35615012]\n",
            " [0.52819968]\n",
            " [1.12015902]] , b =  [0.7358077]\n",
            "step =  33200 , loss value =  6.233696604817348 , W =  [[0.35613511]\n",
            " [0.52820981]\n",
            " [1.12017314]] , b =  [0.73502549]\n",
            "step =  33600 , loss value =  6.233543575356141 , W =  [[0.35612152]\n",
            " [0.52821918]\n",
            " [1.1201866 ]] , b =  [0.73424339]\n",
            "step =  34000 , loss value =  6.233390605503051 , W =  [[0.35610923]\n",
            " [0.52822787]\n",
            " [1.12019948]] , b =  [0.73346141]\n",
            "step =  34400 , loss value =  6.2332376929920805 , W =  [[0.3560981 ]\n",
            " [0.52823594]\n",
            " [1.12021182]] , b =  [0.73267954]\n",
            "step =  34800 , loss value =  6.2330848359651485 , W =  [[0.35608803]\n",
            " [0.52824346]\n",
            " [1.12022367]] , b =  [0.73189779]\n",
            "step =  35200 , loss value =  6.23293203289825 , W =  [[0.35607892]\n",
            " [0.52825047]\n",
            " [1.12023509]] , b =  [0.73111614]\n",
            "step =  35600 , loss value =  6.232779282540833 , W =  [[0.35607067]\n",
            " [0.52825702]\n",
            " [1.12024611]] , b =  [0.73033462]\n",
            "step =  36000 , loss value =  6.232626583866352 , W =  [[0.35606321]\n",
            " [0.52826316]\n",
            " [1.12025678]] , b =  [0.72955321]\n",
            "step =  36400 , loss value =  6.2324739360316 , W =  [[0.35605646]\n",
            " [0.52826892]\n",
            " [1.12026712]] , b =  [0.72877192]\n",
            "step =  36800 , loss value =  6.232321338343477 , W =  [[0.35605034]\n",
            " [0.52827434]\n",
            " [1.12027716]] , b =  [0.72799074]\n",
            "step =  37200 , loss value =  6.232168790231913 , W =  [[0.35604481]\n",
            " [0.52827946]\n",
            " [1.12028694]] , b =  [0.72720969]\n",
            "step =  37600 , loss value =  6.232016291227422 , W =  [[0.35603981]\n",
            " [0.52828429]\n",
            " [1.12029649]] , b =  [0.72642874]\n",
            "step =  38000 , loss value =  6.231863840943072 , W =  [[0.35603528]\n",
            " [0.52828888]\n",
            " [1.12030581]] , b =  [0.72564792]\n",
            "step =  38400 , loss value =  6.231711439059281 , W =  [[0.35603117]\n",
            " [0.52829324]\n",
            " [1.12031494]] , b =  [0.72486722]\n",
            "step =  38800 , loss value =  6.231559085311825 , W =  [[0.35602746]\n",
            " [0.52829739]\n",
            " [1.12032389]] , b =  [0.72408663]\n",
            "step =  39200 , loss value =  6.231406779481681 , W =  [[0.3560241 ]\n",
            " [0.52830135]\n",
            " [1.12033267]] , b =  [0.72330616]\n",
            "step =  39600 , loss value =  6.231254521386926 , W =  [[0.35602106]\n",
            " [0.52830515]\n",
            " [1.12034131]] , b =  [0.72252581]\n",
            "step =  40000 , loss value =  6.231102310875965 , W =  [[0.35601831]\n",
            " [0.52830879]\n",
            " [1.12034982]] , b =  [0.72174558]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaV_wJ8zWRRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbecae79-5d17-4fbd-b428-84a511282a1d"
      },
      "source": [
        "test_data = np.array([100, 98, 81])\r\n",
        "predict(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([178.84617432])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMAmLgg2JFTK"
      },
      "source": [
        "#Example 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGFO1arLJJZT",
        "outputId": "499816f8-9879-4fab-85cd-bdcf203b35bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_data = np.loadtxt('sps.csv', delimiter=',', dtype=np.float32)\r\n",
        "\r\n",
        "x_data = loaded_data[:, 1:]\r\n",
        "t_data = loaded_data[:, [0]]\r\n",
        "\r\n",
        "print('loaded_data.ndim = ', loaded_data.ndim, ', loaded_data.shape = ', loaded_data.shape)\r\n",
        "print('x_data.ndim = ', x_data.ndim, ', x_data.shape = ', x_data.shape)\r\n",
        "print('t_data.ndim = ', t_data.ndim, ', t_data.shape = ', t_data.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded_data.ndim =  2 , loaded_data.shape =  (50, 5)\n",
            "x_data.ndim =  2 , x_data.shape =  (50, 4)\n",
            "t_data.ndim =  2 , t_data.shape =  (50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lBy7RiSMihl",
        "outputId": "40edf856-b752-4553-c7cc-5a03e2dbf7f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(0)\r\n",
        "\r\n",
        "W = np.random.rand(4, 1)\r\n",
        "b = np.random.rand(1, 1)\r\n",
        "\r\n",
        "print('W = ', W, ', W.shape = ', W.shape, ', b = ', b, ', b.shape = ', b.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W =  [[0.5488135 ]\n",
            " [0.71518937]\n",
            " [0.60276338]\n",
            " [0.54488318]] , W.shape =  (4, 1) , b =  [[0.4236548]] , b.shape =  (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSwYLWuuNIc5"
      },
      "source": [
        "def loss_func(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def error_val(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def predict(x):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyFggaiQPfev",
        "outputId": "f6647261-e112-446e-a4b3-f03624629aa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "learning_rate = 1e-3\r\n",
        "\r\n",
        "f = lambda x : loss_func(x_data, t_data)\r\n",
        "\r\n",
        "print('initial error value = ', error_val(x_data, t_data), ', initial W = ', W, '\\n', ', b = ', b)\r\n",
        "\r\n",
        "start_time = datetime.now()\r\n",
        "\r\n",
        "for step in range(30001):\r\n",
        "  W -= learning_rate * numerical_derivative(f, W)\r\n",
        "  b -= learning_rate * numerical_derivative(f, b)\r\n",
        "\r\n",
        "  if (step % 500 == 0):\r\n",
        "    print('step = ', step, 'error value = ', error_val(x_data, t_data))\r\n",
        "\r\n",
        "end_time = datetime.now()\r\n",
        "\r\n",
        "print('')\r\n",
        "print('Elapsed time => ', end_time - start_time)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial error value =  64.38302549674624 , initial W =  [[0.5488135 ]\n",
            " [0.71518937]\n",
            " [0.60276338]\n",
            " [0.54488318]] \n",
            " , b =  [[0.4236548]]\n",
            "step =  0 error value =  60.604776072341444\n",
            "step =  500 error value =  0.01167190429026227\n",
            "step =  1000 error value =  0.004047845907167936\n",
            "step =  1500 error value =  0.0014043526649038332\n",
            "step =  2000 error value =  0.00048722369128301977\n",
            "step =  2500 error value =  0.00016903654707396402\n",
            "step =  3000 error value =  5.864524808192556e-05\n",
            "step =  3500 error value =  2.0346281216250795e-05\n",
            "step =  4000 error value =  7.0589037112171215e-06\n",
            "step =  4500 error value =  2.4490038781355694e-06\n",
            "step =  5000 error value =  8.496531813563435e-07\n",
            "step =  5500 error value =  2.9477720922949027e-07\n",
            "step =  6000 error value =  1.0226949652836696e-07\n",
            "step =  6500 error value =  3.5481202727676135e-08\n",
            "step =  7000 error value =  1.230978727513882e-08\n",
            "step =  7500 error value =  4.2707363649495105e-09\n",
            "step =  8000 error value =  1.4816819081469759e-09\n",
            "step =  8500 error value =  5.14052165565946e-10\n",
            "step =  9000 error value =  1.7834437166114527e-10\n",
            "step =  9500 error value =  6.187448869478846e-11\n",
            "step =  10000 error value =  2.1466628382414438e-11\n",
            "step =  10500 error value =  7.447595024911832e-12\n",
            "step =  11000 error value =  2.583855772566766e-12\n",
            "step =  11500 error value =  8.964384646654429e-13\n",
            "step =  12000 error value =  3.1100881478795545e-13\n",
            "step =  12500 error value =  1.0790086188891214e-13\n",
            "step =  13000 error value =  3.7434938904037783e-14\n",
            "step =  13500 error value =  1.2987613159065e-14\n",
            "step =  14000 error value =  4.5059000875171345e-15\n",
            "step =  14500 error value =  1.5632692166840009e-15\n",
            "step =  15000 error value =  5.423579204531311e-16\n",
            "step =  15500 error value =  1.8816472159910655e-16\n",
            "step =  16000 error value =  6.528154409141282e-17\n",
            "step =  16500 error value =  2.2648666703984575e-17\n",
            "step =  17000 error value =  7.857690960008845e-18\n",
            "step =  17500 error value =  2.726133322193127e-18\n",
            "step =  18000 error value =  9.458002967175832e-19\n",
            "step =  18500 error value =  3.2813425922981097e-19\n",
            "step =  19000 error value =  1.1384228300504383e-19\n",
            "step =  19500 error value =  3.9496353646178484e-20\n",
            "step =  20000 error value =  1.370281435831186e-20\n",
            "step =  20500 error value =  4.754125218582716e-21\n",
            "step =  21000 error value =  1.6494531600977032e-21\n",
            "step =  21500 error value =  5.723172523227641e-22\n",
            "step =  22000 error value =  1.9859337893496504e-22\n",
            "step =  22500 error value =  6.893889116469676e-23\n",
            "step =  23000 error value =  2.3940569803507154e-23\n",
            "step =  23500 error value =  8.318580669294951e-24\n",
            "step =  24000 error value =  2.8926538754400545e-24\n",
            "step =  24500 error value =  1.007017095562386e-24\n",
            "step =  25000 error value =  3.5189776953551637e-25\n",
            "step =  25500 error value =  1.2368469252287204e-25\n",
            "step =  26000 error value =  4.4692299850792143e-26\n",
            "step =  26500 error value =  1.692665607750921e-26\n",
            "step =  27000 error value =  6.981567293526183e-27\n",
            "step =  27500 error value =  3.257963561703718e-27\n",
            "step =  28000 error value =  1.7847429561181468e-27\n",
            "step =  28500 error value =  1.1894827601256653e-27\n",
            "step =  29000 error value =  9.176150809230155e-28\n",
            "step =  29500 error value =  7.812023703927484e-28\n",
            "step =  30000 error value =  7.296706880293302e-28\n",
            "\n",
            "Elapsed time =>  0:00:06.732070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYK0XKKPT1SR",
        "outputId": "8a4a9002-1530-4756-9e68-2ec53a63da9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ex_data_01 = np.array([4, 4, 4, 4])\r\n",
        "print('predicted value = ', predict(ex_data_01))\r\n",
        "\r\n",
        "ex_data_02 = np.array([-3, 0, 9, -1])\r\n",
        "print('predicted value = ', predict(ex_data_02))\r\n",
        "\r\n",
        "ex_data_03 = np.array([-7, -9, -2, 8])\r\n",
        "print('predicted value = ', predict(ex_data_03))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted value =  [[-3.21117277e-14]]\n",
            "predicted value =  [[7.]]\n",
            "predicted value =  [[-8.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yve8Ho_8Y_Gg"
      },
      "source": [
        "#Example 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmnsox6ZBJ2",
        "outputId": "28a1ada1-01ba-444b-ebe4-b6e51f49e90d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\r\n",
        "from datetime import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "try:\r\n",
        "  loaded_data = np.loadtxt('sps.csv', delimiter=',', dtype=np.float32)\r\n",
        "\r\n",
        "  x_data = loaded_data[:, 1:]\r\n",
        "  t_data = loaded_data[:, [0]]\r\n",
        "\r\n",
        "  print('loaded_data.ndim = ', loaded_data.ndim, ', loaded_data.shape = ', loaded_data.shape)\r\n",
        "  print('x_data.ndim = ', x_data.ndim, ', x_data.shape = ', x_data.shape)\r\n",
        "  print('t_data.ndim = ', t_data.ndim, ', t_data.shape = ', t_data.shape)\r\n",
        "\r\n",
        "except FileNotFoundError as err:\r\n",
        "  print(str(err))\r\n",
        "except IndexError as err:\r\n",
        "  print(str(err))\r\n",
        "except Exception as err:\r\n",
        "  print(str(err))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded_data.ndim =  2 , loaded_data.shape =  (50, 5)\n",
            "x_data.ndim =  2 , x_data.shape =  (50, 4)\n",
            "t_data.ndim =  2 , t_data.shape =  (50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsInAei2a5W6",
        "outputId": "9303bf75-3dfa-4ead-b5b4-48f2f05e6d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(0)\r\n",
        "\r\n",
        "W = np.random.rand(4, 1)\r\n",
        "b = np.random.rand(1, 1)\r\n",
        "\r\n",
        "print('W = ', W, ', W.shape = ', W.shape, ', b = ', b, ', b.shape = ', b.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W =  [[0.5488135 ]\n",
            " [0.71518937]\n",
            " [0.60276338]\n",
            " [0.54488318]] , W.shape =  (4, 1) , b =  [[0.4236548]] , b.shape =  (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkdhxWavbSkh"
      },
      "source": [
        "def loss_func(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def error_val(x, t):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return (np.sum((t - y)**2)) / (len(x))\r\n",
        "\r\n",
        "def predict(x):\r\n",
        "  y = np.dot(x,W) + b\r\n",
        "  return y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl12tIeibW_W",
        "outputId": "c6c12142-7ec0-43ec-b62a-3679a1d365a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "learning_rate = 1e-3\r\n",
        "\r\n",
        "f = lambda x : loss_func(x_data, t_data)\r\n",
        "\r\n",
        "print('initial error value = ', error_val(x_data, t_data), ', initial W = ', W, '\\n', ', inital b = ', b)\r\n",
        "\r\n",
        "loss_val_list = []\r\n",
        "\r\n",
        "start_time = datetime.now()\r\n",
        "\r\n",
        "for step in range(40001):\r\n",
        "  W -= learning_rate * numerical_derivative(f, W)\r\n",
        "  b -= learning_rate * numerical_derivative(f, b)\r\n",
        "\r\n",
        "  if (step % 5000 == 0):\r\n",
        "    print('step == ', step, ', error value = ', error_val(x_data, t_data))\r\n",
        "    loss_val_list.append(error_val(x_data, t_data))\r\n",
        "\r\n",
        "end_time = datetime.now()\r\n",
        "\r\n",
        "print('')\r\n",
        "print('Elapsed time = ', end_time - start_time)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial error value =  64.38302549674624 , initial W =  [[0.5488135 ]\n",
            " [0.71518937]\n",
            " [0.60276338]\n",
            " [0.54488318]] \n",
            " , inital b =  [[0.4236548]]\n",
            "step ==  0 , error value =  60.604776072341444\n",
            "step ==  5000 , error value =  8.496531813563435e-07\n",
            "step ==  10000 , error value =  2.1466628382414438e-11\n",
            "step ==  15000 , error value =  5.423579204531311e-16\n",
            "step ==  20000 , error value =  1.370281435831186e-20\n",
            "step ==  25000 , error value =  3.5189776953551637e-25\n",
            "step ==  30000 , error value =  7.296706880293302e-28\n",
            "step ==  35000 , error value =  6.4101916533982895e-28\n",
            "step ==  40000 , error value =  6.410191720247789e-28\n",
            "\n",
            "Elapsed time =  0:00:08.605696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LbQI7IndIle",
        "outputId": "f09ba77b-8c75-4f3c-93fb-d7e4d86e9242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.title('Loss Value Trend')\r\n",
        "plt.xlabel('epoches ( X 5000)')\r\n",
        "plt.ylabel('loss value')\r\n",
        "plt.grid()\r\n",
        "\r\n",
        "plt.plot(loss_val_list, ls='--', lw=2, color='r', label='lr=le-3 , epoch=40,000')\r\n",
        "plt.legend(loc='best')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkB8pPJefNW2"
      },
      "source": [
        "#Example 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXqPgeu6fO1j"
      },
      "source": [
        "import numpy as np\r\n",
        "from datetime import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "np.random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zep3lTvKflWG"
      },
      "source": [
        "class LinearRegressionTest:\r\n",
        "\r\n",
        "  def __init__(self, xdata, tdata, learning_rate, iteration_count):\r\n",
        "    self.xdata = xdata\r\n",
        "    self.tdata = tdata\r\n",
        "\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.iteration_count = iteration_count\r\n",
        "\r\n",
        "    self.W = np.random.rand(self.xdata.shape[1], 1)\r\n",
        "    self.b = np.random.rand(1)\r\n",
        "\r\n",
        "    print('LinearRegressionTest Object is created')\r\n",
        "  \r\n",
        "  def get_W_b(self):\r\n",
        "    return self.W, self.b\r\n",
        "  \r\n",
        "  def loss_func(self):\r\n",
        "    y = np.dot(self.xdata, self.W) + self.b\r\n",
        "    return (np.sum((self.tdata - y)**2)) / (len(self.xdata))\r\n",
        "  \r\n",
        "  def error_val(self):\r\n",
        "    y = np.dot(self.xdata, self.W) + self.b\r\n",
        "    return (np.sum((self.tdata - y)**2)) / (len(self.xdata))\r\n",
        "  \r\n",
        "  def predict(self, test_data):\r\n",
        "    y = np.dot(test_data, self.W) + self.b\r\n",
        "    return y\r\n",
        "\r\n",
        "  def train(self):\r\n",
        "    f = lambda x : self.loss_func()\r\n",
        "\r\n",
        "    start_time = datetime.now()\r\n",
        "\r\n",
        "    for step in range(self.iteration_count):\r\n",
        "      self.W -= self.learning_rate * numerical_derivative(f, self.W)\r\n",
        "      self.b -= self.learning_rate * numerical_derivative(f, self.b)\r\n",
        "\r\n",
        "      if (step % 1000 == 0):\r\n",
        "        print('step = ', step, 'error value = ', self.error_val())\r\n",
        "    \r\n",
        "    end_time = datetime.now()\r\n",
        "\r\n",
        "    print('')\r\n",
        "    print('Elapsed time => ', end_time - start_time)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpJ0WjZlvJp",
        "outputId": "4df63af7-15b9-42e4-deba-29bc0684a6f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\r\n",
        "  loaded_data = np.loadtxt('sps.csv', delimiter=',', dtype=np.float32)\r\n",
        "\r\n",
        "  x_data = loaded_data[:, 1:]\r\n",
        "  t_data = loaded_data[:, [0]]\r\n",
        "\r\n",
        "  print('x_data.ndim = ', x_data.ndim, ', x_data.shape = ', x_data.shape)\r\n",
        "  print('t_data.ndim = ', t_data.ndim, ', t_data.shape = ', t_data.shape)\r\n",
        "\r\n",
        "except FileNotFoundError as err:\r\n",
        "  print(str(err))\r\n",
        "except Exception as err:\r\n",
        "  print(str(err))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data.ndim =  2 , x_data.shape =  (50, 4)\n",
            "t_data.ndim =  2 , t_data.shape =  (50, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0JvThL5mVHM",
        "outputId": "1dbb6da2-577a-4c21-cbe4-7e341c26da50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "obj1 = LinearRegressionTest(x_data, t_data, 1e-3, 20001)\r\n",
        "obj1.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearRegressionTest Object is created\n",
            "step =  0 error value =  60.604776072341444\n",
            "step =  1000 error value =  0.004047845907167936\n",
            "step =  2000 error value =  0.00048722369128301977\n",
            "step =  3000 error value =  5.864524808192556e-05\n",
            "step =  4000 error value =  7.0589037112171215e-06\n",
            "step =  5000 error value =  8.496531813563435e-07\n",
            "step =  6000 error value =  1.0226949652836696e-07\n",
            "step =  7000 error value =  1.230978727513882e-08\n",
            "step =  8000 error value =  1.4816819081469759e-09\n",
            "step =  9000 error value =  1.7834437166114527e-10\n",
            "step =  10000 error value =  2.1466628382414438e-11\n",
            "step =  11000 error value =  2.583855772566766e-12\n",
            "step =  12000 error value =  3.1100881478795545e-13\n",
            "step =  13000 error value =  3.7434938904037783e-14\n",
            "step =  14000 error value =  4.5059000875171345e-15\n",
            "step =  15000 error value =  5.423579204531311e-16\n",
            "step =  16000 error value =  6.528154409141282e-17\n",
            "step =  17000 error value =  7.857690960008845e-18\n",
            "step =  18000 error value =  9.458002967175832e-19\n",
            "step =  19000 error value =  1.1384228300504383e-19\n",
            "step =  20000 error value =  1.370281435831186e-20\n",
            "\n",
            "Elapsed time =>  0:00:04.464284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grb6OfBYmlVS",
        "outputId": "c76df28c-d9c7-48df-b461-70c7d5afd414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_data = np.array([[4,4,4,4], [-3,0,9,-1], [-7,-9,-2,8], [1,-2,3,-2], [19,-12,0,-76],[2001,-1,109,31], [-1,102,-200,1000]])\r\n",
        "\r\n",
        "for data in test_data:\r\n",
        "  print(obj1.predict(data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.68604748e-11]\n",
            "[7.]\n",
            "[-8.]\n",
            "[8.]\n",
            "[107.]\n",
            "[2079.99999996]\n",
            "[-1303.00000001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6F20DKbnLWt",
        "outputId": "0616cf24-0329-424b-f0ff-fbb3576c6ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(obj1.get_W_b())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[ 1.],\n",
            "       [-1.],\n",
            "       [ 1.],\n",
            "       [-1.]]), array([1.58863405e-10]))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}